{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uzVGHY1FTGYq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)  # 0 = empty, 1 = player 1 (X), -1 = player 2 (O)\n",
        "        self.current_winner = None\n",
        "\n",
        "    def print_board(self):\n",
        "        # Convert the board to a human-readable format\n",
        "        symbols = {1: 'X', -1: 'O', 0: ' '}\n",
        "        for row in self.board:\n",
        "            print('| ' + ' | '.join([symbols[cell] for cell in row]) + ' |')\n",
        "\n",
        "    def available_moves(self):\n",
        "        # Returns a list of available moves (i.e., empty squares)\n",
        "        return [(r, c) for r in range(3) for c in range(3) if self.board[r, c] == 0]\n",
        "\n",
        "    def make_move(self, row, col, player):\n",
        "        # Place the player's symbol on the board if the move is valid\n",
        "        if self.board[row, col] == 0:\n",
        "            self.board[row, col] = player\n",
        "            if self.check_winner(player):\n",
        "                self.current_winner = player\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_winner(self, player):\n",
        "        # Check rows, columns, and diagonals for a win\n",
        "        for row in self.board:\n",
        "            if np.all(row == player):\n",
        "                return True\n",
        "        for col in self.board.T:\n",
        "            if np.all(col == player):\n",
        "                return True\n",
        "        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def is_full(self):\n",
        "        # Check if the board is full (draw)\n",
        "        return not np.any(self.board == 0)\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the board for a new game\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_winner = None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(board, player):\n",
        "    # A feature vector for the current state\n",
        "    features = np.zeros(5)  # Example: 5 simple features\n",
        "\n",
        "    # Feature 1: Number of two-in-a-row (for player)\n",
        "    features[0] = count_two_in_a_row(board, player)\n",
        "\n",
        "    # Feature 2: Number of two-in-a-row (for opponent)\n",
        "    features[1] = count_two_in_a_row(board, -player)\n",
        "\n",
        "    # Feature 3: Number of empty spaces\n",
        "    features[2] = len(np.where(board == 0)[0])\n",
        "\n",
        "    # Feature 4: Number of player 1 marks\n",
        "    features[3] = np.sum(board == player)\n",
        "\n",
        "    # Feature 5: Number of opponent marks\n",
        "    features[4] = np.sum(board == -player)\n",
        "\n",
        "    return features\n",
        "\n",
        "def count_two_in_a_row(board, player):\n",
        "    # Helper function to count two-in-a-rows\n",
        "    two_in_row = 0\n",
        "\n",
        "    # Check rows, columns, and diagonals for two-in-a-row with one empty space\n",
        "    for row in board:\n",
        "        if np.sum(row == player) == 2 and np.sum(row == 0) == 1:\n",
        "            two_in_row += 1\n",
        "    for col in board.T:\n",
        "        if np.sum(col == player) == 2 and np.sum(col == 0) == 1:\n",
        "            two_in_row += 1\n",
        "    diag1 = np.diag(board)\n",
        "    diag2 = np.diag(np.fliplr(board))\n",
        "    if np.sum(diag1 == player) == 2 and np.sum(diag1 == 0) == 1:\n",
        "        two_in_row += 1\n",
        "    if np.sum(diag2 == player) == 2 and np.sum(diag2 == 0) == 1:\n",
        "        two_in_row += 1\n",
        "\n",
        "    return two_in_row\n"
      ],
      "metadata": {
        "id": "kDJoVsIITKvf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, player, learning_rate=0.01, discount=0.9, epsilon=0.1):\n",
        "        self.player = player\n",
        "        self.weights = np.random.randn(5)  # Initialize random weights for the features\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def q_value(self, features):\n",
        "        # Linear function approximation for Q-value\n",
        "        return np.dot(self.weights, features)\n",
        "\n",
        "    def select_action(self, game):\n",
        "        # Epsilon-greedy policy for action selection\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.choice(game.available_moves())  # Explore\n",
        "        else:\n",
        "            # Exploit: select the action with the highest Q-value\n",
        "            available_moves = game.available_moves()\n",
        "            q_values = []\n",
        "            for move in available_moves:\n",
        "                # Simulate the move and get features for the resulting state\n",
        "                game.board[move] = self.player\n",
        "                features = get_features(game.board, self.player)\n",
        "                q_values.append(self.q_value(features))\n",
        "                game.board[move] = 0  # Undo the move\n",
        "\n",
        "            return available_moves[np.argmax(q_values)]\n",
        "\n",
        "    def update(self, state_features, reward, next_state_features):\n",
        "        # Update weights using the Q-learning rule\n",
        "        current_q = self.q_value(state_features)\n",
        "        next_q = self.q_value(next_state_features)\n",
        "        td_error = reward + self.gamma * next_q - current_q\n",
        "        self.weights += self.lr * td_error * state_features\n"
      ],
      "metadata": {
        "id": "xKUhIHAKTMo4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def play_game(agent1, agent2, game):\n",
        "    game.reset()\n",
        "    current_agent = agent1  # Player 1 starts\n",
        "    next_agent = agent2\n",
        "\n",
        "    while True:\n",
        "        move = current_agent.select_action(game)\n",
        "        game.make_move(*move, current_agent.player)\n",
        "\n",
        "        if game.current_winner:\n",
        "            return current_agent.player  # The current agent wins\n",
        "\n",
        "        if game.is_full():\n",
        "            return 0\n",
        "\n",
        "        current_agent, next_agent = next_agent, current_agent\n",
        "\n",
        "agent1 = QLearningAgent(1)\n",
        "agent2 = QLearningAgent(-1)\n",
        "\n",
        "num_games = 10000\n",
        "win_counts = {1: 0, -1: 0, 0: 0}\n",
        "\n",
        "for i in range(num_games):\n",
        "    result = play_game(agent1, agent2, TicTacToe())\n",
        "    win_counts[result] += 1\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Games played: {i+1}, Win counts: {win_counts}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWp9SW_yTOHH",
        "outputId": "8140f8ee-f60e-449f-aca9-fea1313b8661"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Games played: 1000, Win counts: {1: 198, -1: 107, 0: 695}\n",
            "Games played: 2000, Win counts: {1: 397, -1: 222, 0: 1381}\n",
            "Games played: 3000, Win counts: {1: 601, -1: 327, 0: 2072}\n",
            "Games played: 4000, Win counts: {1: 807, -1: 435, 0: 2758}\n",
            "Games played: 5000, Win counts: {1: 1036, -1: 549, 0: 3415}\n",
            "Games played: 6000, Win counts: {1: 1256, -1: 644, 0: 4100}\n",
            "Games played: 7000, Win counts: {1: 1471, -1: 746, 0: 4783}\n",
            "Games played: 8000, Win counts: {1: 1692, -1: 855, 0: 5453}\n",
            "Games played: 9000, Win counts: {1: 1930, -1: 961, 0: 6109}\n",
            "Games played: 10000, Win counts: {1: 2147, -1: 1058, 0: 6795}\n"
          ]
        }
      ]
    }
  ]
}